# Jupyter Notebook

# ---
# Reconhecimento de Sinais em LIBRAS com Landmarks de Mãos
# ---

# # 1. Instalação e Imports

# Instale as dependências (execute apenas se necessário)
# !pip install opencv-python mediapipe pandas numpy tensorflow pyarrow scikit-learn

import pandas as pd
import numpy as np
from src.dataset_utils import load_metadata, load_landmarks
from src.model import build_transformer_model
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

# # 2. Carregar Metadados

metadata = load_metadata('../data/train.csv')
print(metadata.head())

# # 3. Codificar Labels

label_encoder = LabelEncoder()
metadata['label_encoded'] = label_encoder.fit_transform(metadata['label'])
num_classes = len(label_encoder.classes_)
print("Classes:", label_encoder.classes_)

# # 4. Carregar Landmarks e Preparar Dados

MAX_FRAMES = 60  # Ajuste conforme a duração média dos seus vídeos

X = []
y = []

for _, row in metadata.iterrows():
    df = load_landmarks(row['path'])
    df = df.fillna(0)
    arr = df.drop(columns=['frame'], errors='ignore').values
    # Padronizar para MAX_FRAMES frames
    if len(arr) >= MAX_FRAMES:
        arr = arr[:MAX_FRAMES]
    else:
        arr = np.pad(arr, ((0, MAX_FRAMES - len(arr)), (0, 0)))
    X.append(arr)
    y.append(row['label_encoded'])

X = np.stack(X)
y = to_categorical(y, num_classes=num_classes)

print("Shape X:", X.shape)
print("Shape y:", y.shape)

# # 5. Dividir em Treino e Validação

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Treino:", X_train.shape, "Validação:", X_val.shape)

# # 6. Construir e Compilar o Modelo

input_shape = X_train.shape[1:]  # (frames, features)
model = build_transformer_model(input_shape, num_classes)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# # 7. Treinar o Modelo

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=8
)

# # 8. Avaliar o Modelo

plt.plot(history.history['accuracy'], label='Treino')
plt.plot(history.history['val_accuracy'], label='Validação')
plt.xlabel('Época')
plt.ylabel('Acurácia')
plt.legend()
plt.show()

# # 9. Salvar o Modelo e o Encoder

model.save('../model_libras.h5')
import pickle
with open('../label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# # 10. Inferência em Novos Vídeos

def predict_landmarks(landmarks_array):
    arr = landmarks_array
    if len(arr) >= MAX_FRAMES:
        arr = arr[:MAX_FRAMES]
    else:
        arr = np.pad(arr, ((0, MAX_FRAMES - len(arr)), (0, 0)))
    arr = np.expand_dims(arr, axis=0)
    pred = model.predict(arr)
    label_idx = np.argmax(pred)
    return label_encoder.inverse_transform([label_idx])[0]

# Exemplo de uso:
# df_new = load_landmarks('CAMINHO/DO/SEU/ARQUIVO.parquet')
# print(predict_landmarks(df_new.drop(columns=['frame'], errors='ignore').values))