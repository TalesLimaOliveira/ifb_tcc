# ğŸŒŸ Trabalho de ConclusÃ£o de Curso - Instituto Federal de BrasÃ­lia ğŸŒŸ

## RECONHECIMENTO E TRADUÃ‡ÃƒO DE FRASES EM LIBRAS UTILIZANDO REDES NEURAIS

<p align="center">
    <img src="https://img.shields.io/badge/Language-Python-blue?logo=python" alt="Language">
    <img src="https://img.shields.io/badge/Language-Julia-purple?logo=julia" alt="Language">
    <img src="https://img.shields.io/badge/Status-Active-success" alt="Status">
</p>


## ğŸ‘¨â€ğŸ« Orientador
- **Raimundo Vasconcelos**

## ğŸ‘¨â€ğŸ“ Aluno
- **[Tales Oliveira](https://github.com/TalesLimaOliveira)**

---

## ğŸ“š DescriÃ§Ã£o
Este projeto tem como objetivo desenvolver uma rede neural profunda que utiliza tÃ©cnicas de visÃ£o computacional para o reconhecimento e traduÃ§Ã£o de gestos e frases completas em LIBRAS (LÃ­ngua Brasileira de Sinais). Diferentemente de soluÃ§Ãµes existentes que se concentram na traduÃ§Ã£o de sinais ou letras isoladas, esta proposta busca criar um sistema que compreenda e traduza frases de forma contÃ­nua, e em tempo real, levando em consideraÃ§Ã£o o contexto dos sinais previamente realizados.

# LIBRAS Sign Recognition

Reconhecimento e traduÃ§Ã£o de sinais em LIBRAS (LÃ­ngua Brasileira de Sinais) usando visÃ£o computacional e redes neurais profundas.

## Objetivo

Desenvolver um sistema capaz de reconhecer e traduzir gestos e frases completas em LIBRAS a partir de vÃ­deos, utilizando MediaPipe para extraÃ§Ã£o de landmarks e TensorFlow para classificaÃ§Ã£o, com traduÃ§Ã£o em tempo real via webcam.

## Estrutura do Projeto

- `data/raw_videos/`: vÃ­deos originais em .mp4
- `data/landmarks/`: arquivos .parquet com landmarks extraÃ­dos
- `data/train.csv`: metadados para treinamento
- `model/src/`: scripts Python para extraÃ§Ã£o, preparaÃ§Ã£o e modelagem
- `notebooks/`: notebooks para experimentaÃ§Ã£o e treinamento
- `saved_models/`: modelos treinados e encoders salvos
- `app/`: aplicaÃ§Ã£o Streamlit para traduÃ§Ã£o em tempo real

## Como treinar o modelo

1. **Crie um ambiente virtual:**
   ```
   py -3.10 -m venv venv
   venv\Scripts\activate
   ```

2. **Instale as dependÃªncias:**
   ```
   pip install -r requirements.txt
   ```

3. **Extraia os landmarks dos vÃ­deos:**
   ```
   python model/src/extract_landmarks.py --input_dir data/raw_videos --output_dir data/landmarks
   ```

4. **Prepare o arquivo `train.csv` com os metadados.**

5. **Treine o modelo:**
   - Abra e execute o notebook `notebooks/train_model.ipynb`.
   - O modelo treinado (`model_libras.h5`) e o encoder (`label_encoder.pkl`) serÃ£o salvos em `saved_models/`.

## Como usar a traduÃ§Ã£o em tempo real

1. **Execute o app Streamlit:**
   ```
   streamlit run app/app.py
   ```

2. **Clique em "Iniciar Webcam" para comeÃ§ar a traduÃ§Ã£o dos sinais capturados pela cÃ¢mera.**

O modelo treinado serÃ¡ carregado automaticamente de `saved_models/`.

---

## ObservaÃ§Ãµes

- Certifique-se de que os arquivos `model_libras.h5` e `label_encoder.pkl` estejam presentes em `saved_models/` antes de rodar o app.
- O prÃ©-processamento dos dados deve ser idÃªntico no treinamento e na aplicaÃ§Ã£o em tempo real para garantir bons resultados.